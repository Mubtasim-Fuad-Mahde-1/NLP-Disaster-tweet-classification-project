# -*- coding: utf-8 -*-
"""CSE440 Project Visualization

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B6EVQD4stPcaev1eckTgHkLs3Mj5gzH5
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re
import nltk
import keras
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from keras.models import Sequential
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras import backend as K


nltk.download('stopwords')
nltk.download('wordnet')

# from google.colab import drive
#drive.mount('/content/drive')

# from google.colab import files
# uploaded = files.upload()


from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/CSE440/CSE440_data.csv')

df = df.drop(['id', 'keyword', 'location'], axis=1)

df

# prompt: train test split the df dataframe with test size = 0.1 and seed = 42

from sklearn.model_selection import train_test_split

train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)

# prompt: find number of unique words in tranin_df

unique_words = set()
for text in train_df['text']:
  for word in text.split():
    unique_words.add(word)

num_unique_words = len(unique_words)
print("Number of unique words:", num_unique_words)

# prompt: number of unique words with frequency more than 1

from collections import Counter

word_counts = Counter()
for text in train_df['text']:
  for word in text.split():
    word_counts[word] += 1

unique_words_freq_more_than_1 = sum(1 for count in word_counts.values() if count > 1)
print("Number of unique words with frequency more than 1:", unique_words_freq_more_than_1)

# prompt: bar chart of how many instances in each class "target", label 1 as disaster and 0 as not disaster give exact number too along the graph

target_counts = df['target'].value_counts()

plt.figure(figsize=(8, 6))
bars = plt.bar(target_counts.index, target_counts.values, color=['lightgreen', 'salmon'])
plt.xlabel('Target')
plt.ylabel('Number of Instances')
plt.title('Distribution of Target Classes')
plt.xticks([0, 1], ['0', '1'])

for bar in bars:
  yval = bar.get_height()
  plt.text(bar.get_x() + bar.get_width()/2, yval, yval, ha='center', va='bottom')


plt.show()

# prompt: frequency of top 40 unique words that are in Target=1 but not in target=0 and vice versa

# Filter the DataFrame to include only rows where target=1
disaster_tweets = df[df['target'] == 1]['text']

# Create a list of all words in disaster tweets
all_words_disaster = []
for tweet in disaster_tweets:
  words = tweet.lower().split()
  all_words_disaster.extend(words)

# Calculate the frequency distribution of words
word_counts_disaster = Counter(all_words_disaster)

# Filter the DataFrame to include only rows where target=0
not_disaster_tweets = df[df['target'] == 0]['text']

# Create a list of all words in not disaster tweets
all_words_not_disaster = []
for tweet in not_disaster_tweets:
  words = tweet.lower().split()
  all_words_not_disaster.extend(words)

# Calculate the frequency distribution of words
word_counts_not_disaster = Counter(all_words_not_disaster)

# Find words unique to disaster tweets
unique_disaster_words = {word: count for word, count in word_counts_disaster.items() if word not in word_counts_not_disaster}
# Find words unique to not disaster tweets
unique_not_disaster_words = {word: count for word, count in word_counts_not_disaster.items() if word not in word_counts_disaster}


# Get the top 30 most frequent words for disaster tweets
top_30_unique_disaster_words = dict(sorted(unique_disaster_words.items(), key=lambda item: item[1], reverse=True)[:30])
# Get the top 20 most frequent words for not disaster tweets
top_0_unique_not_disaster_words = dict(sorted(unique_not_disaster_words.items(), key=lambda item: item[1], reverse=True)[:30])

# Create a bar chart
plt.figure(figsize=(12, 6))
plt.bar(top_30_unique_disaster_words.keys(), top_30_unique_disaster_words.values())
plt.xlabel("Words")
plt.ylabel("Frequency")
plt.title("Frequency Distribution of Top 30  Words Unique to class 1")
plt.xticks(rotation=45, ha='right')

# Add exact numbers to the bars
for i, (word, count) in enumerate(top_30_unique_disaster_words.items()):
  plt.text(i, count, str(count), ha='center', va='bottom')

plt.show()


# Create a bar chart
plt.figure(figsize=(12, 6))
plt.bar(top_30_unique_not_disaster_words.keys(), top_30_unique_not_disaster_words.values())
plt.xlabel("Words")
plt.ylabel("Frequency")
plt.title("Frequency Distribution of Top 30  Words Unique to class 0")
plt.xticks(rotation=45, ha='right')

# Add exact numbers to the bars
for i, (word, count) in enumerate(top_30_unique_not_disaster_words.items()):
  plt.text(i, count, str(count), ha='center', va='bottom')

plt.show()



# prompt: maximum number of characters in an instance in the entire data frame

max_chars = df['text'].str.len().max()
print(f"Maximum number of characters in an instance: {max_chars}")

# prompt: no ou unique words and unique characters in the entire dtaframe

# Combine all text data into a single string
all_text = ' '.join(df['text'].tolist())

# Calculate the number of unique words
unique_words = len(set(all_text.lower().split()))

# Calculate the number of unique characters
unique_chars = len(set(all_text))

print(f"Number of unique words: {unique_words}")
print(f"Number of unique characters: {unique_chars}")

df.groupby("target").describe()

# prompt: 5000 most frequently used words in df as a word cloud

from wordcloud import WordCloud
from collections import Counter

# Combine all text data into a single string
all_text = ' '.join(df['text'].tolist())

# Tokenize the text into words
words = all_text.lower().split()

# Count word frequencies
word_counts = Counter(words)

# Get the 5000 most frequent words
top_5000_words = dict(word_counts.most_common(5000))

# Generate the word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(top_5000_words)

# Display the word cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()



from sklearn.model_selection import train_test_split

X = df['text']
y = df['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42) #randomly selecting data points to split with seed/pattern selected as 42










import re

def sanitize_text(text):
    text = str(text)
    text = text.lower()
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    text = re.sub(r'\d+', '', text)  # Remove numbers
    text = re.sub(r'(.)\1{2,}', r'\1', text)  # Remove characters repeated more than twice
    text = re.sub(r'\s+', ' ', text).strip()  # Replace multiple spaces with a single space
    return text

X_train = X_train.apply(sanitize_text)
X_test = X_test.apply(sanitize_text)
X_train

# prompt: print all numbers in X_train

import re

for text in X_train:
  numbers = re.findall(r'\d+', text)
  if numbers:
    print(numbers)



# prompt: number of words unique to target=1 in entire df and in X_train

# Entire DataFrame
disaster_tweets_df = df[df['target'] == 1]['text']
all_words_disaster_df = []
for tweet in disaster_tweets_df:
  words = tweet.lower().split()
  all_words_disaster_df.extend(words)

word_counts_disaster_df = Counter(all_words_disaster_df)
not_disaster_tweets_df = df[df['target'] == 0]['text']
all_words_not_disaster_df = []
for tweet in not_disaster_tweets_df:
  words = tweet.lower().split()
  all_words_not_disaster_df.extend(words)

word_counts_not_disaster_df = Counter(all_words_not_disaster_df)
unique_disaster_words_df = {word: count for word, count in word_counts_disaster_df.items() if word not in word_counts_not_disaster_df}
num_unique_words_df = len(unique_disaster_words_df)
print(f"Number of unique words for target=1 in the entire DataFrame: {num_unique_words_df}")


# X_train
disaster_tweets_train = X_train[y_train == 1]
all_words_disaster_train = []
for tweet in disaster_tweets_train:
  words = tweet.lower().split()
  all_words_disaster_train.extend(words)

word_counts_disaster_train = Counter(all_words_disaster_train)
not_disaster_tweets_train = X_train[y_train == 0]
all_words_not_disaster_train = []
for tweet in not_disaster_tweets_train:
  words = tweet.lower().split()
  all_words_not_disaster_train.extend(words)

word_counts_not_disaster_train = Counter(all_words_not_disaster_train)
unique_disaster_words_train = {word: count for word, count in word_counts_disaster_train.items() if word not in word_counts_not_disaster_train}
num_unique_words_train = len(unique_disaster_words_train)
print(f"Number of unique words for target=1 in X_train: {num_unique_words_train}")

df = pd.read_csv('/content/drive/MyDrive/440/CSE440_data.csv')

df = df.drop(['id', 'keyword', 'location'], axis=1)

df




from sklearn.model_selection import train_test_split

X = df['text']
y = df['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42) #randomly selecting data points to split with seed/pattern selected as 42










import re

def sanitize_text(text):
    text = str(text)
    text = text.lower()
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    text = re.sub(r'\d+', '', text)  # Remove numbers
    text = re.sub(r'(.)\1{2,}', r'\1', text)  # Remove characters repeated more than twice
    text = re.sub(r'\s+', ' ', text).strip()  # Replace multiple spaces with a single space
    return text

X_train = X_train.apply(sanitize_text)
X_test = X_test.apply(sanitize_text)
X_train

# prompt: barchart of Top 30 words that are in in X_train who's Y_train=1 but not in Y_train=0, and vice versa

# Filter the DataFrame to include only rows where target=1
disaster_tweets_train = X_train[y_train == 1]

# Create a list of all words in disaster tweets
all_words_disaster_train = []
for tweet in disaster_tweets_train:
  words = tweet.lower().split()
  all_words_disaster_train.extend(words)

# Calculate the frequency distribution of words
word_counts_disaster_train = Counter(all_words_disaster_train)


# Filter the DataFrame to include only rows where target=0
not_disaster_tweets_train = X_train[y_train == 0]

# Create a list of all words in not disaster tweets
all_words_not_disaster_train = []
for tweet in not_disaster_tweets_train:
  words = tweet.lower().split()
  all_words_not_disaster_train.extend(words)

# Calculate the frequency distribution of words
word_counts_not_disaster_train = Counter(all_words_not_disaster_train)


# Find words unique to disaster tweets
unique_disaster_words_train = {word: count for word, count in word_counts_disaster_train.items() if word not in word_counts_not_disaster_train}
# Find words unique to not disaster tweets
unique_not_disaster_words_train = {word: count for word, count in word_counts_not_disaster_train.items() if word not in word_counts_disaster_train}



# Get the top 30 most frequent words for disaster tweets
top_30_unique_disaster_words_train = dict(sorted(unique_disaster_words_train.items(), key=lambda item: item[1], reverse=True)[:30])
# Get the top 30 most frequent words for not disaster tweets
top_30_unique_not_disaster_words_train = dict(sorted(unique_not_disaster_words_train.items(), key=lambda item: item[1], reverse=True)[:30])



# Create a bar chart
plt.figure(figsize=(12, 6))
plt.bar(top_30_unique_disaster_words_train.keys(), top_30_unique_disaster_words_train.values())
plt.xlabel("Words")
plt.ylabel("Frequency")
plt.title("Frequency Distribution of Top 30  Words Unique to class 1 in X_train")
plt.xticks(rotation=45, ha='right')

# Add exact numbers to the bars
for i, (word, count) in enumerate(top_30_unique_disaster_words_train.items()):
  plt.text(i, count, str(count), ha='center', va='bottom')

plt.show()


# Create a bar chart
plt.figure(figsize=(12, 6))
plt.bar(top_30_unique_not_disaster_words_train.keys(), top_30_unique_not_disaster_words_train.values())
plt.xlabel("Words")
plt.ylabel("Frequency")
plt.title("Frequency Distribution of Top 30  Words Unique to class 0 in X_train")
plt.xticks(rotation=45, ha='right')

# Add exact numbers to the bars
for i, (word, count) in enumerate(top_30_unique_not_disaster_words_train.items()):
  plt.text(i, count, str(count), ha='center', va='bottom')

plt.show()

# prompt: barchart of Top 30 words that are in in df  who's target=1 but not in target=0, and vice versa

# Filter the DataFrame to include only rows where target=1
disaster_tweets_df = df[df['target'] == 1]['text']

# Create a list of all words in disaster tweets
all_words_disaster_df = []
for tweet in disaster_tweets_df:
    words = tweet.lower().split()
    all_words_disaster_df.extend(words)

# Calculate the frequency distribution of words
word_counts_disaster_df = Counter(all_words_disaster_df)

# Filter the DataFrame to include only rows where target=0
not_disaster_tweets_df = df[df['target'] == 0]['text']

# Create a list of all words in not disaster tweets
all_words_not_disaster_df = []
for tweet in not_disaster_tweets_df:
    words = tweet.lower().split()
    all_words_not_disaster_df.extend(words)

# Calculate the frequency distribution of words
word_counts_not_disaster_df = Counter(all_words_not_disaster_df)

# Find words unique to disaster tweets
unique_disaster_words_df = {word: count for word, count in word_counts_disaster_df.items() if word not in word_counts_not_disaster_df}
# Find words unique to not disaster tweets
unique_not_disaster_words_df = {word: count for word, count in word_counts_not_disaster_df.items() if word not in word_counts_disaster_df}


# Get the top 30 most frequent words for disaster tweets
top_30_unique_disaster_words_df = dict(sorted(unique_disaster_words_df.items(), key=lambda item: item[1], reverse=True)[:30])
# Get the top 30 most frequent words for not disaster tweets
top_30_unique_not_disaster_words_df = dict(sorted(unique_not_disaster_words_df.items(), key=lambda item: item[1], reverse=True)[:30])



# Create a bar chart
plt.figure(figsize=(12, 6))
plt.bar(top_30_unique_disaster_words_df.keys(), top_30_unique_disaster_words_df.values())
plt.xlabel("Words")
plt.ylabel("Frequency")
plt.title("Frequency Distribution of Top 30  Words Unique to class 1 in df")
plt.xticks(rotation=45, ha='right')

# Add exact numbers to the bars
for i, (word, count) in enumerate(top_30_unique_disaster_words_df.items()):
    plt.text(i, count, str(count), ha='center', va='bottom')

plt.show()


# Create a bar chart
plt.figure(figsize=(12, 6))
plt.bar(top_30_unique_not_disaster_words_df.keys(), top_30_unique_not_disaster_words_df.values())
plt.xlabel("Words")
plt.ylabel("Frequency")
plt.title("Frequency Distribution of Top 30  Words Unique to class 0 in df")
plt.xticks(rotation=45, ha='right')

# Add exact numbers to the bars
for i, (word, count) in enumerate(top_30_unique_not_disaster_words_df.items()):
    plt.text(i, count, str(count), ha='center', va='bottom')

plt.show()

# prompt: total number of times "panicking" appeared in df
# total number of times "panicking" appeared in df where target=1
# total number of times "panicking" appeared in df where target=0
# total number of times "panicking" appeared in X_train where target=1
# total number of times "panicking" appeared in X_train where target=0
# in what indexes of df target=1, did the word "panicking" appear?

# Total number of times "panicking" appeared in df
total_panicking_df = df['text'].str.contains('panicking', case=False).sum()
print(f"Total number of times 'panicking' appeared in df: {total_panicking_df}")
print()
# Total number of times "panicking" appeared in df where target=1
total_panicking_df_target1 = df[df['target'] == 1]['text'].str.contains('panicking', case=False).sum()
print(f"Total number of times 'panicking' appeared in df where target=1: {total_panicking_df_target1}")

# Total number of times "panicking" appeared in df where target=0
total_panicking_df_target0 = df[df['target'] == 0]['text'].str.contains('panicking', case=False).sum()
print(f"Total number of times 'panicking' appeared in df where target=0: {total_panicking_df_target0}")
print()

# Total number of times "panicking" appeared in X_train where target=1
total_panicking_train_target1 = X_train[y_train == 1].str.contains('panicking', case=False).sum()
print(f"Total number of times 'panicking' appeared in X_train where target=1: {total_panicking_train_target1}")


# Total number of times "panicking" appeared in X_train where target=0
total_panicking_train_target0 = X_train[y_train == 0].str.contains('panicking', case=False).sum()
print(f"Total number of times 'panicking' appeared in X_train where target=0: {total_panicking_train_target0}")
print()

# In what indexes of df target=1, did the word "panicking" appear?
indexes_panicking_target1 = df[(df['target'] == 1) & (df['text'].str.contains('panicking', case=False))].index.tolist()
print(f"Indexes of df where target=1 and 'panicking' appears: {indexes_panicking_target1}")

# prompt: max number of words in a tweet in X_train and in df

# Maximum number of words in a tweet in X_train
max_words_X_train = X_train.str.split().str.len().max()
print(f"Maximum number of words in a tweet in X_train: {max_words_X_train}")

# Maximum number of words in a tweet in df
max_words_df = df['text'].str.split().str.len().max()
print(f"Maximum number of words in a tweet in df: {max_words_df}")

# prompt: max character count in a tweet in X_train and df

# Maximum number of characters in a tweet in X_train
max_chars_X_train = X_train.str.len().max()
print(f"Maximum number of characters in a tweet in X_train: {max_chars_X_train}")

# Maximum number of characters in a tweet in df
max_chars_df = df['text'].str.len().max()
print(f"Maximum number of characters in a tweet in df: {max_chars_df}")